---
layout: page
title: About me
permalink: /bio/
---

<img src="https://ati.sh/images/atish_headshot.jpg" alt="Atish Agarwala" width="300" />

I am currently a physics PhD student at Stanford University advised by 
[Daniel S. Fisher](https://web.stanford.edu/group/dsfisher/index.html).
Previously I studied math and physics as an undergraduate at Swarthmore College.

My research area is, broadly speaking, theoretical biology. During my PhD, I've primarily studied evolution - trying
to understand how different dynamical processes combine to give evolutionary dynamics, and to characterize things like the
speeed and predictability of evolution. I've worked on analyzing data from experimental evolution, and developed robust 
methods
of inferring fitness from abundance data (code [here](https://github.com/barcoding-bfa/fitness-assay-python)). I used my
code to understand the nature of fitness gains in glucose limited yeast (in collaboration with experimentalists from the 
Petrov and Sherlock labs at Stanford).

Most of my work consists of building mathematical models of different evolutionary scenarios, and trying to understand them
both quantitatively and qualitatively. I studied evolution in the presence of epistasis (interactions
between the effects of mutations) by developing a class of _random fitness landscapes_ to model the complexities of fitness
in real biological systems. I derived a computational and analytical framework to understand the stochastic
dynamics quantitatively in the low-mutation rate regime, and showed how evolution now is heavily conditioned on past 
evolution. I'm currently working at the intersection of ecology and evolution, trying to understand how host-pathogen
interactions and spatial structure might stabilize the within-species diversity found in even well-mixed microbial
ecosystems.

I'm also interested in theoretical neuroscience and theoretical machine learning. I want to understand the scaling
laws of neural systems - how various geometric and dyamical quantities change with network size, neural statistics, learning
rates, and other network/dynamical parameters. In 2018 I did a research internship at
Google Brain, studying early learning dynamics with mean field theory to try and understand the performance of
different initialization statistics.

For more on my background, check out this
[interview](https://stanfordcehg.wordpress.com/2018/11/19/fellows-feature-atish-agarwala/)
conducted as part of my CEHG fellowship.

Links:

[Google scholar](https://scholar.google.com/citations?user=yCeAZUoAAAAJ&hl=en)

[CV](https://ati.sh/atishagarwalacv.pdf)
